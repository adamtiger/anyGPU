
Model on huggingface:
    stabilityai/stable-video-diffusion-img2vid-xt

Implementation:
    https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_video_diffusion/pipeline_stable_video_diffusion.py

Details:

    Main parts of the model:
        vae: AutoencoderKLTemporalDecoder,
        image_encoder: CLIPVisionModelWithProjection,
        unet: UNetSpatioTemporalConditionModel,
        scheduler: EulerDiscreteScheduler,
        feature_extractor: CLIPImageProcessor
	
	    self.video_processor = VideoProcessor(do_resize=True, vae_scale_factor=self.vae_scale_factor)
	
	SVD functions:
	    _encode_image
		    self.feature_extractor
			self.image_encoder
		
		_encode_vae_image
		    self.vae.encode
		
	    decode_latents
		    for cycle {self.vae.decode}
		
		__call__:
		    self._encode_image
			self.video_processor.preprocess
			self._encode_vae_image
			self.prepare_latents (simple function)
			
			for cycle 
			{
			    self.unet
				self.scheduler.step
				self.decode_latents
				self.video_processor.postprocess_video
			}

	VAE functions: link(https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/autoencoders/autoencoder_kl_temporal_decoder.py#L165)
	    self.encoder = Encoder
		self.decoder = TemporalDecoder
		self.quant_conv = nn.Conv2d
		
		func encode
	        self.encoder
			self.quant_conv
			DiagonalGaussianDistribution
		
		func decode
		    self.decoder
		
		func forward (most likely not necessary!)
		    self.encode
			self.decode
	
	Encoder in VAE functions: link(https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/autoencoders/vae.py#L60)
	    self.conv_in = nn.Conv2d
		self.down_blocks = list(get_down_block)
		self.mid_block = UNetMidBlock2D
		self.conv_norm_out = nn.GroupNorm
		self.conv_act = nn.SiLU
		self.conv_out = nn.Conv2d
		
		func forward (follows the same order)
		
	TemporalDecoder in VAE functions: link(https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/autoencoders/autoencoder_kl_temporal_decoder.py#L30)
	    self.conv_in = nn.Conv2d
		self.mid_block = MidBlockTemporalDecoder
		self.up_blocks = list(UpBlockTemporalDecoder)
		self.conv_norm_out = nn.GroupNorm
		self.conv_act = nn.SiLU
        self.conv_out = torch.nn.Conv2d
		self.time_conv_out = torch.nn.Conv3d
		
		func forward (follows the same order)


MidBlockTemporalDecoder
	
	MidBlockTemporalDecoder in TemporalDecoder in VAE functions: link(https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unets/unet_3d_blocks.py#L899)
	    self.resnets = list(SpatioTemporalResBlock)
		self.attentions = list(Attention)
	
	Attention in MidBlockTemporalDecoder: link(https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py#L50)
	    normal attention block with several variants (normalizations, liner etc.)
	
	SpatioTemporalResBlock in MidBlockTemporalDecoder: link(https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/resnet.py#L635)
	    self.spatial_res_block = ResnetBlock2D
		self.temporal_res_block = TemporalResnetBlock
		self.time_mixer = AlphaBlender
		
		func forward (follows the same order; reshape, permute)
	
	ResnetBlock2D in SpatioTemporalResBlock: link(https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/resnet.py#L189)
	    depends on the settings but the main parts:
		
		self.norm1 = torch.nn.GroupNorm
		self.conv1 = nn.Conv2d
		self.time_emb_proj = nn.Linear
		self.norm2 = torch.nn.GroupNorm
		self.dropout = torch.nn.Dropout
		self.conv2 = nn.Conv2d
		self.nonlinearity = get_activation
		self.upsample = Upsample2D or self.downsample = Downsample2D
		self.conv_shortcut = nn.Conv2d
		
		func forward 
	
	Upsample2D in ResnetBlock2D: link(https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/upsampling.py#L76)
	    self.norm = nn.LayerNorm or self.norm = RMSNorm
		conv = nn.ConvTranspose2d or conv = nn.Conv2d
		
		func forward
	
	Downsample2D in ResnetBlock2D: link(https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/downsampling.py#L69)
	    self.norm = nn.LayerNorm or self.norm = RMSNorm
		conv = nn.Conv2d or conv = nn.AvgPool2d
		
	TemporalResnetBlock in SpatioTemporalResBlock: link(https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/resnet.py#L542)
	    self.norm1 = torch.nn.GroupNorm
		self.conv1 = nn.Conv3d
		self.time_emb_proj = nn.Linear
		self.norm2 = torch.nn.GroupNorm
		self.dropout = torch.nn.Dropout
		self.conv2 = nn.Conv3d
		self.nonlinearity = get_activation
		self.conv_shortcut = nn.Conv3d
		
		func forward (similar order but some permute and adding)
	
	AlphaBlender in SpatioTemporalResBlock: link(https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/resnet.py#L719)
	    func forward (simple scaling)


UpBlockTemporalDecoder
    
	UpBlockTemporalDecoder in TemporalDecoder: link(https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unets/unet_3d_blocks.py#L962)
	    self.resnets = list(SpatioTemporalResBlock)
		self.upsamplers = list(Upsample2D)


UNetMidBlock2D
    
	UNetMidBlock2D in Encoder: link(https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unets/unet_2d_blocks.py#L589)
	    Attention in attentions (if requested in settings)
		resnets = list(ResnetBlock2D) or resnets = list(ResnetBlockCondNorm2D)
	
	ResnetBlockCondNorm2D in UNetMidBlock2D: link(https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/resnet.py#L44)
	    AdaGroupNorm or SpatialNorm
		self.conv1 = nn.Conv2d
		AdaGroupNorm or SpatialNorm
		self.dropout = torch.nn.Dropout
		self.conv2 = nn.Conv2d
		self.nonlinearity = get_activation
		Upsample2D or Downsample2D
		self.conv_shortcut = nn.Conv2d
	
	SpatialNorm in ResnetBlockCondNorm2D: link(https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py#L4812)
	AdaGroupNorm in ResnetBlockCondNorm2D: link(https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/normalization.py#L271)


# CLIPImageProcessor
    
	CLIPImageProcessor functions: link(https://github.com/huggingface/transformers/blob/main/src/transformers/models/clip/image_processing_clip.py#L52)
        preprocess function is the key; in BaseImageProcessor the __call__ function calls it
		link: https://github.com/huggingface/transformers/blob/main/src/transformers/image_processing_utils.py#L35
		
		resize or center_crop or rescale or normalize


# CLIPVisionModelWithProjection
    
	CLIPVisionModelWithProjection functions: link(https://github.com/huggingface/transformers/blob/main/src/transformers/models/clip/modeling_clip.py#L1510)
        vision_model = CLIPVisionModel._from_config(config)
        self.vision_model = vision_model.vision_model
        self.visual_projection = nn.Linear
		
		CLIPVisionTransformer is inside: link(https://github.com/huggingface/transformers/blob/main/src/transformers/models/clip/modeling_clip.py#L1066)
		    self.embeddings = CLIPVisionEmbeddings(config) (https://github.com/huggingface/transformers/blob/main/src/transformers/models/clip/modeling_clip.py#L177)
            self.pre_layrnorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)
            self.encoder = CLIPEncoder(config) -> CLIPEncoderLayer (https://github.com/huggingface/transformers/blob/main/src/transformers/models/clip/modeling_clip.py#L585)
            self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)


# UNetSpatioTemporalConditionModel
    
	UNetSpatioTemporalConditionModel functions: link(https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unets/unet_spatio_temporal_condition.py#L32)
	    self.conv_in = nn.Conv2d
		self.time_proj = Timesteps
		self.time_embedding = TimestepEmbedding
		self.add_time_proj = Timesteps
		self.add_embedding = TimestepEmbedding
		
		self.down_blocks = list(CrossAttnDownBlockSpatioTemporal x 3, DownBlockSpatioTemporal)
		self.mid_block = UNetMidBlockSpatioTemporal
		self.up_blocks = list(UpBlockSpatioTemporal, CrossAttnUpBlockSpatioTemporal x 3)
		
		self.conv_norm_out = nn.GroupNorm
		self.conv_act = nn.SiLU
		self.conv_out = nn.Conv2d


## UNetMidBlockSpatioTemporal
	UNetMidBlockSpatioTemporal in UNetSpatioTemporalConditionModel: link(https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unets/unet_3d_blocks.py#L1012)
	    self.attentions = list(TransformerSpatioTemporalModel)
		self.resnets = list(SpatioTemporalResBlock)  # already identified
		
	TransformerSpatioTemporalModel in UNetMidBlockSpatioTemporal: link(https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/transformers/transformer_temporal.py#L205)
	    self.norm = torch.nn.GroupNorm
		self.proj_in = nn.Linear
		self.transformer_blocks = list(BasicTransformerBlock)
		self.temporal_transformer_blocks = list(TemporalBasicTransformerBlock)
		self.time_pos_embed = TimestepEmbedding
		self.time_proj = Timesteps
		self.time_mixer = AlphaBlender
		self.proj_out = nn.Linear
	
	BasicTransformerBlock in TransformerSpatioTemporalModel: link(https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention.py#L261)
	    depends on settings
	
	Timesteps in TransformerSpatioTemporalModel: link(https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/embeddings.py#L1317)
	    requires standalone implementation
	
	TimestepEmbedding in TransformerSpatioTemporalModel: link(https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/embeddings.py#L1269)
	    two linears and activation
	
	TemporalBasicTransformerBlock in TransformerSpatioTemporalModel: link(https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention.py#L643)
        self.norm_in = nn.LayerNorm
		self.ff_in = FeedForward
		self.norm1 = nn.LayerNorm
		self.attn1 = Attention
		# if requested:
		self.norm2 = nn.LayerNorm
        self.attn2 = Attention
		
		self.norm3 = nn.LayerNorm
		self.ff = FeedForward
	
	FeedForward in TemporalBasicTransformerBlock: link(https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention.py#L1192)
	    depends on settings


## CrossAttnDownBlockSpatioTemporal
    CrossAttnDownBlockSpatioTemporal in UNetSpatioTemporalConditionModel: link(https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unets/unet_3d_blocks.py#L1166)
	    self.attentions = list(TransformerSpatioTemporalModel)
		self.resnets = list(SpatioTemporalResBlock)
		self.downsamplers = list(Downsample2D)  # if given

## CrossAttnUpBlockSpatioTemporal
    CrossAttnUpBlockSpatioTemporal in UNetSpatioTemporalConditionModel: link(https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unets/unet_3d_blocks.py#L1332)
	    self.attentions = list(TransformerSpatioTemporalModel)
		self.resnets = list(SpatioTemporalResBlock)
		self.upsamplers = list(Upsample2D)  # if given

## DownBlockSpatioTemporal
    DownBlockSpatioTemporal in UNetSpatioTemporalConditionModel: link(https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unets/unet_3d_blocks.py#L1101)
        self.resnets = list(SpatioTemporalResBlock)
		self.downsamplers = list(Downsample2D)

## UpBlockSpatioTemporal
    UpBlockSpatioTemporal in UNetSpatioTemporalConditionModel: link(https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unets/unet_3d_blocks.py#L1267)
	    self.resnets = list(SpatioTemporalResBlock)
		self.upsamplers = list(Upsample2D)



# EulerDiscreteScheduler functions

    EulerDiscreteScheduler functions: link(https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_euler_discrete.py#L135)
